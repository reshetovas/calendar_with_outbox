package broker

import (
	"calendar/pkg/config"
	"context"
	"fmt"
	"strings"
	"time"

	"go.uber.org/zap"

	"github.com/IBM/sarama"
)

const (
	_consumerGroup = "consumer-group"
)

type KafkaBroker struct {
	ConsumerTopic string
	ProducerTopic string
	ConsumerGroup sarama.ConsumerGroup
	SyncProducer  sarama.SyncProducer
	Brokers       []string
	conf          config.Kafka
	logger        *zap.SugaredLogger
}

func NewKafkaBroker(conf config.Kafka, logger *zap.SugaredLogger) (*KafkaBroker, error) {
	logger.Debugf("–°–æ–∑–¥–∞–Ω–∏–µ consumer group –¥–ª—è brokers: %s\n", conf.Brokers)
	consumerGroup, err := newConsumerGroup(conf)
	if err != nil {
		logger.Errorf("–û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è consumer group: %v\n", err)
		return nil, fmt.Errorf("%w", err)
	}
	logger.Infof("Consumer group —Å–æ–∑–¥–∞–Ω —É—Å–ø–µ—à–Ω–æ\n")

	logger.Debugf("–°–æ–∑–¥–∞–Ω–∏–µ producer –¥–ª—è brokers: %s\n", conf.Brokers)
	syncProducer, err := newSyncProducer(conf)
	if err != nil {
		logger.Errorf("–û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è producer: %v\n", err)
		return nil, fmt.Errorf("%w", err)
	}
	logger.Infof("Producer —Å–æ–∑–¥–∞–Ω —É—Å–ø–µ—à–Ω–æ\n")

	brokers := strings.Split(conf.Brokers, ",")
	broker := &KafkaBroker{
		ConsumerTopic: conf.ReaderTopic,
		ProducerTopic: conf.WriterTopic,
		ConsumerGroup: consumerGroup,
		SyncProducer:  syncProducer,
		Brokers:       brokers,
		conf:          conf,
		logger:        logger,
	}
	logger.Infof("KafkaBroker —Å–æ–∑–¥–∞–Ω. Consumer topic: %s, Producer topic: %s\n", broker.ConsumerTopic, broker.ProducerTopic)
	return broker, nil
}

// HealthCheck –ø—Ä–æ–≤–µ—Ä—è–µ—Ç –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å Kafka –±—Ä–æ–∫–µ—Ä–∞, Producer –∏ ConsumerGroup
//
// –í–∞–∂–Ω–æ: –ù–ï –∏—Å–ø–æ–ª—å–∑—É–µ—Ç client.Partitions(), —Ç–∞–∫ –∫–∞–∫ —ç—Ç–æ —Ç—Ä–µ–±—É–µ—Ç –æ–ø–µ—Ä–∞—Ü–∏–∏ Describe –≤ ACL.
// –ï—Å–ª–∏ –Ω–∞ —Å—Ç–µ–Ω–¥–µ –Ω–∞—Å—Ç—Ä–æ–µ–Ω—ã –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è (–Ω–∞–ø—Ä–∏–º–µ—Ä, consumer –¢–£–ó –º–æ–∂–µ—Ç —Ç–æ–ª—å–∫–æ Write,
// –∞ producer –¢–£–ó –º–æ–∂–µ—Ç —Ç–æ–ª—å–∫–æ Read), —Ç–æ –ø—Ä–æ–≤–µ—Ä–∫–∞ Partitions() —Å–ª–æ–º–∞–µ—Ç—Å—è.
//
// –í–º–µ—Å—Ç–æ —ç—Ç–æ–≥–æ –ø—Ä–æ–≤–µ—Ä—è–µ–º:
// 1. –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—é SyncProducer –∏ ConsumerGroup (–µ—Å–ª–∏ –æ–Ω–∏ —Å–æ–∑–¥–∞–Ω—ã - –∑–Ω–∞—á–∏—Ç –ø—Ä–∞–≤–∞ –µ—Å—Ç—å)
// 2. –î–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å –±—Ä–æ–∫–µ—Ä–æ–≤ —á–µ—Ä–µ–∑ –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–π –∫–ª–∏–µ–Ω—Ç (–Ω–µ —Ç—Ä–µ–±—É–µ—Ç Describe)
//
// –ï—Å–ª–∏ –ø—Ä–æ–¥—é—Å–µ—Ä –∏ –∫–æ–Ω—Å—å—é–º–µ—Ä —É—Å–ø–µ—à–Ω–æ —Å–æ–∑–¥–∞–Ω—ã, –∑–Ω–∞—á–∏—Ç –æ–Ω–∏ –∏–º–µ—é—Ç –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ –ø—Ä–∞–≤–∞
// –¥–ª—è —Ä–∞–±–æ—Ç—ã (Write –¥–ª—è producer, Read –¥–ª—è consumer). –ü—Ä–æ–≤–µ—Ä–∫–∞ –±—Ä–æ–∫–µ—Ä–æ–≤ –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–∞–µ—Ç
// –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å Kafka –∫–ª–∞—Å—Ç–µ—Ä–∞.
func (kb *KafkaBroker) HealthCheck(ctx context.Context) error {
	// –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ Producer –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω
	// –ï—Å–ª–∏ SyncProducer —Å–æ–∑–¥–∞–Ω —É—Å–ø–µ—à–Ω–æ - –∑–Ω–∞—á–∏—Ç —É –Ω–µ–≥–æ –µ—Å—Ç—å –ø—Ä–∞–≤–∞ Write –Ω–∞ producer topic
	if kb.SyncProducer == nil {
		return fmt.Errorf("kafka producer is not initialized")
	}

	// –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ ConsumerGroup –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω
	// –ï—Å–ª–∏ ConsumerGroup —Å–æ–∑–¥–∞–Ω —É—Å–ø–µ—à–Ω–æ - –∑–Ω–∞—á–∏—Ç —É –Ω–µ–≥–æ –µ—Å—Ç—å –ø—Ä–∞–≤–∞ Read –Ω–∞ consumer topic
	if kb.ConsumerGroup == nil {
		return fmt.Errorf("kafka consumer group is not initialized")
	}

	// –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å –±—Ä–æ–∫–µ—Ä–æ–≤ —á–µ—Ä–µ–∑ –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–π –∫–ª–∏–µ–Ω—Ç
	// –≠—Ç–æ –Ω–µ —Ç—Ä–µ–±—É–µ—Ç Describe –ø—Ä–∞–≤, —Ç–æ–ª—å–∫–æ –±–∞–∑–æ–≤–æ–µ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏–µ
	cfg := sarama.NewConfig()
	cfg.Net.DialTimeout = 2 * time.Second
	cfg.Net.ReadTimeout = 2 * time.Second
	cfg.Net.WriteTimeout = 2 * time.Second
	cfg.Metadata.Timeout = 2 * time.Second
	cfg.Metadata.Retry.Max = 1

	// –ü—Ä–∏–º–µ–Ω—è–µ–º —Ç–µ –∂–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ SASL, —á—Ç–æ –∏ –≤ producer (–ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç Writer credentials)
	if kb.conf.WriterUsr != "" && kb.conf.WriterUsrPwd != "" {
		applySASLConfig(cfg, kb.conf, true)
	} else {
		applySASLConfig(cfg, kb.conf, false)
	}

	client, err := sarama.NewClient(kb.Brokers, cfg)
	if err != nil {
		return fmt.Errorf("failed to connect to kafka brokers: %w", err)
	}
	defer client.Close()

	// –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å –±—Ä–æ–∫–µ—Ä–æ–≤ (—ç—Ç–æ –Ω–µ —Ç—Ä–µ–±—É–µ—Ç Describe –ø—Ä–∞–≤)
	brokers := client.Brokers()
	if len(brokers) == 0 {
		return fmt.Errorf("no kafka brokers available")
	}

	return nil
}

// applySASLConfig –ø—Ä–∏–º–µ–Ω—è–µ—Ç SASL –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –∫ sarama.Config
// useWriterCreds: true - –∏—Å–ø–æ–ª—å–∑—É–µ—Ç WriterUsr/WriterUsrPwd, false - ReaderUsr/ReaderUsrPwd
func applySASLConfig(cfg *sarama.Config, conf config.Kafka, useWriterCreds bool) {
	if useWriterCreds {
		if conf.WriterUsr != "" && conf.WriterUsrPwd != "" {
			cfg.Net.SASL.User = conf.WriterUsr
			cfg.Net.SASL.Password = conf.WriterUsrPwd
			cfg.Net.SASL.Enable = true
			cfg.Net.SASL.Mechanism = sarama.SASLTypePlaintext
		}
	} else {
		if conf.ReaderUsr != "" && conf.ReaderUsrPwd != "" {
			cfg.Net.SASL.User = conf.ReaderUsr
			cfg.Net.SASL.Password = conf.ReaderUsrPwd
			cfg.Net.SASL.Enable = true
			cfg.Net.SASL.Mechanism = sarama.SASLTypePlaintext
		}
	}
}

func EnableSaramaZapLogs(base *zap.SugaredLogger) {
	logger := base.Named("sarama")
	sarama.Logger = &zapSarama{logger}
	logger.Info("üîß Sarama logger initialized")
	sarama.Logger.Print("üîß Test message from Sarama logger")
}

type zapSarama struct{ l *zap.SugaredLogger }

func (z *zapSarama) Print(v ...interface{})                 { z.l.Debug(v...) }
func (z *zapSarama) Printf(format string, v ...interface{}) { z.l.Debugf(format, v...) }
func (z *zapSarama) Println(v ...interface{})               { z.l.Debug(v...) }

func newConsumerGroup(conf config.Kafka) (sarama.ConsumerGroup, error) {
	kafkaConfig := sarama.NewConfig()
	applySASLConfig(kafkaConfig, conf, false) // –∏—Å–ø–æ–ª—å–∑—É–µ–º Reader credentials

	brokers := strings.Split(conf.Brokers, ",")

	consumer, err := sarama.NewConsumerGroup(brokers, _consumerGroup, kafkaConfig)
	if err != nil {
		return nil, fmt.Errorf("–æ—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ–∑–¥–∞–Ω–∏–∏ Kafka Consumer Group: %w", err)
	}

	return consumer, nil
}

func newSyncProducer(conf config.Kafka) (sarama.SyncProducer, error) {
	kafkaConfig := sarama.NewConfig()

	kafkaConfig.Net.DialTimeout = 10 * time.Second
	kafkaConfig.Net.ReadTimeout = 15 * time.Second
	kafkaConfig.Net.WriteTimeout = 15 * time.Second
	kafkaConfig.Net.KeepAlive = 30 * time.Second

	kafkaConfig.Metadata.Timeout = 10 * time.Second
	kafkaConfig.Metadata.Retry.Max = 1
	kafkaConfig.Metadata.Retry.Backoff = 1 * time.Second
	kafkaConfig.Metadata.RefreshFrequency = 1 * time.Minute

	kafkaConfig.Producer.RequiredAcks = sarama.WaitForAll
	kafkaConfig.Producer.Return.Successes = true
	kafkaConfig.Producer.Return.Errors = true
	kafkaConfig.Producer.Retry.Max = 0
	kafkaConfig.Producer.Timeout = 10 * time.Second
	kafkaConfig.Producer.Partitioner = sarama.NewHashPartitioner

	applySASLConfig(kafkaConfig, conf, true) // –∏—Å–ø–æ–ª—å–∑—É–µ–º Writer credentials

	brokers := strings.Split(conf.Brokers, ",")

	producer, err := sarama.NewSyncProducer(brokers, kafkaConfig)
	if err != nil {
		return nil, fmt.Errorf("–æ—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ–∑–¥–∞–Ω–∏–∏ Kafka Sync Producer: %w", err)
	}

	return producer, nil
}
